# Unsloth Puzzle Solutions

This repository contains my solutions for the Unsloth coding puzzles, which serve as a technical challenge for potential engineering roles at Unsloth. The solutions here demonstrate my work on optimizing kernels, enabling QLoRA with FSDP2, making `torch.compile` work without graph breaks, and other AI/ML-related tasks.

## ğŸš€ Project Overview
This repository covers the following puzzles:
- **Puzzle A**: Convert `nf4` to a Triton kernel.
- **Puzzle B**: Implement QLoRA with FSDP2 on multiple GPUs.
- **Puzzle C**: Optimize `torch.compile` to remove graph breaks in QLoRA.
- **Puzzle D**: Solve Unsloth GitHub issues (e.g., Flex Attention, VLMs support, GGUF Vision support).
- **Puzzle E**: Implement memory-efficient backpropagation for large vocabulary LLMs.

The repository is structured for easy navigation, testing, and submission.

## ğŸ“‚ Repository Structure
```
my-unsloth-solutions/
â”œâ”€â”€ README.md  # This file
â”œâ”€â”€ environment.yml  # Conda environment file (optional)
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ notebooks/
â”‚    â”œâ”€â”€ Unsloth_Puzzles.ipynb  # (Optional) Original challenge notebook
â”‚    â”œâ”€â”€ puzzle_A_triton_dequant.ipynb
â”‚    â”œâ”€â”€ puzzle_B_fsdp2_qlora.ipynb
â”‚    â”œâ”€â”€ puzzle_C_torch_compile.ipynb
â”‚    â”œâ”€â”€ puzzle_D_unsloth_issues.md
â”‚    â””â”€â”€ puzzle_E_mem_efficient_backprop.ipynb
â””â”€â”€ src/
     â”œâ”€â”€ my_triton_kernel.py  # Triton kernel for Puzzle A
     â”œâ”€â”€ my_fsdp2_script.py  # Training script for Puzzle B
     â””â”€â”€ ...
```

## ğŸ›  Installation
### Local Setup
To run the notebooks locally, clone the repository and install the necessary dependencies:
```sh
git clone https://github.com/YOUR_GITHUB_USERNAME/my-unsloth-solutions.git
cd my-unsloth-solutions
pip install -r requirements.txt
```
Alternatively, if using Conda:
```sh
conda env create -f environment.yml
conda activate unsloth-env
```
Ensure you have a **GPU with CUDA 11.4+** (Tesla T4 recommended).

## ğŸ“Š Running the Solutions
Each puzzle has its own notebook/script:

### **Puzzle A: Convert `nf4` to Triton**
```sh
cd notebooks
jupyter notebook puzzle_A_triton_dequant.ipynb
```
- Run `test_dequantize(your_dequantize_nf4)` to check performance.
- Verify speedup (`test_dequantize(unsloth_dequantize) / test_dequantize(your_dequantize_nf4)`).

### **Puzzle B: QLoRA with FSDP2**
- Run the Kaggle notebook (link below) to showcase multi-GPU finetuning on 2x Tesla T4 GPUs.
- Ensure the loss matches single-GPU results.

### **Puzzle C: Optimize `torch.compile`**
- Ensure there are **no graph breaks** (`TORCHDYNAMO_VERBOSE=1`).
- Show the training loss curve to validate correctness.

### **Puzzle D: Solve Unsloth GitHub Issues**
- Links to pull requests and implemented features are included in `puzzle_D_unsloth_issues.md`.

### **Puzzle E: Memory-Efficient Backpropagation**
- Implemented using `torch.autograd.Function`.
- Should reduce VRAM usage while keeping gradients numerically equivalent.

## ğŸ”— Kaggle / Colab Notebooks
| Puzzle | Notebook Link |
|--------|--------------|
| Puzzle B | [FSDP2 + QLoRA on Kaggle](https://www.kaggle.com/username/my-fsdp2-qlora) |
| Puzzle C | [Torch.compile Optimization](https://www.kaggle.com/username/my-torch-compile-qlora) |
| Puzzle E | [Memory-Efficient Backprop](https://www.kaggle.com/username/my-mem-efficient-backprop) |

## ğŸ“¸ Results & Screenshots
- Speedup comparison for Puzzle A.
- Training loss curves for Puzzles B & C.
- VRAM reduction analysis for Puzzle E.

## âœ… Submission Details
- **GitHub Repo:** [https://github.com/YOUR_GITHUB_USERNAME/my-unsloth-solutions](https://github.com/YOUR_GITHUB_USERNAME/my-unsloth-solutions)
- **Kaggle/Colab Links:** Provided above.
- **Submission Form:** Submitted via [Unsloth puzzle form](https://forms.gle/crSYnsGq3t1ck5TB9)

## ğŸ† Final Notes
This repository is my attempt to achieve the **Founding Engineer / ML Engineer** qualification by scoring **47+ points** through these solutions. Feedback from Unsloth maintainers is appreciated!

